{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PolyFuzz \u00b6 PolyFuzz performs fuzzy string matching, string grouping, and contains extensive evaluation functions. PolyFuzz is meant to bring fuzzy string matching techniques together within a single framework. Currently, methods include Levenshtein distance with RapidFuzz, a character-based n-gram TF-IDF, word embedding techniques such as FastText and GloVe, and \ud83e\udd17 transformers embeddings. The philosophy of PolyFuzz is: Easy to use yet highly customizable . It is a string matcher tool that requires only a few lines of code but that allows you customize and create your own models. Installation \u00b6 You can install PolyFuzz via pip: pip install polyfuzz This will install the base dependencies and excludes any deep learning/embedding models. If you want to be making use of \ud83e\udd17 Transformers, install the additional additional Flair dependency: pip install polyfuzz[flair]","title":"Home"},{"location":"#polyfuzz","text":"PolyFuzz performs fuzzy string matching, string grouping, and contains extensive evaluation functions. PolyFuzz is meant to bring fuzzy string matching techniques together within a single framework. Currently, methods include Levenshtein distance with RapidFuzz, a character-based n-gram TF-IDF, word embedding techniques such as FastText and GloVe, and \ud83e\udd17 transformers embeddings. The philosophy of PolyFuzz is: Easy to use yet highly customizable . It is a string matcher tool that requires only a few lines of code but that allows you customize and create your own models.","title":"PolyFuzz"},{"location":"#installation","text":"You can install PolyFuzz via pip: pip install polyfuzz This will install the base dependencies and excludes any deep learning/embedding models. If you want to be making use of \ud83e\udd17 Transformers, install the additional additional Flair dependency: pip install polyfuzz[flair]","title":"Installation"},{"location":"releases/","text":"v0.2.2 - Update grouping to include all strings only if identical lists of strings are compared v0.2.0 - Update naming convention matcher --> model - Update documentation - Add basic models to grouper - Fix issues with vector order in cosine similarity - Update naming of cosine similarity function v0.1.0 - Additional tests - More thorough documentation - Prepare for public release v0.0.1 First release of PolyFuzz Matching through: Edit Distance TF-IDF Embeddings Custom models Grouping of results with custom models Evaluation through precision-recall curves","title":"Releases"},{"location":"api/linkage/","text":"polyfuzz.linkage \u00b6 single_linkage ( matches , min_similarity = 0.8 ) \u00b6 Show source code in polyfuzz\\linkage.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def single_linkage ( matches : pd . DataFrame , min_similarity : float = 0.8 ) -> Tuple [ Mapping [ int , List [ str ]], Mapping [ str , int ], Mapping [ str , str ]]: \"\"\" Single linkage clustering from column 'From' to column 'To' `matches` contains three columns: *From*, *To*, and *Similarity* where *Similarity* is already the minimum similarity score and thus no checking for minimum similarity is necessary. Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for creating groups min_similarity: minimum similarity between strings before they can be merged into a group Returns: clusters: The populated clusters cluster_mapping: The mapping from a string to a cluster cluster_name_map: The mapping from a string to the representative string in its respective cluster \"\"\" matches = matches . loc [ matches . Similarity > min_similarity , :] cluster_mapping = {} cluster_id = 0 for row in matches . itertuples (): # If from string has not already been mapped if not cluster_mapping . get ( row . From ): # If the to string has not already been mapped if not cluster_mapping . get ( row . To ): cluster_mapping [ row . To ] = cluster_id cluster_mapping [ row . From ] = cluster_id cluster_id += 1 # If the to string has already been mapped else : cluster_mapping [ row . From ] = cluster_mapping . get ( row . To ) # Populate the clusters clusters = {} for key , value in cluster_mapping . items (): clusters . setdefault ( value , []) clusters [ value ] . append ( key ) cluster_name_map = { key : clusters . get ( value )[ 0 ] for key , value in cluster_mapping . items ()} return clusters , cluster_mapping , cluster_name_map Single linkage clustering from column 'From' to column 'To' matches contains three columns: From , To , and Similarity where Similarity is already the minimum similarity score and thus no checking for minimum similarity is necessary. Parameters Name Type Description Default matches DataFrame contains the columns From , To , and Similarity used for creating groups required min_similarity float minimum similarity between strings before they can be merged into a group 0.8 Returns Type Description Tuple[Mapping[int, List[str]], Mapping[str, int], Mapping[str, str]] clusters: The populated clusters cluster_mapping: The mapping from a string to a cluster cluster_name_map: The mapping from a string to the representative string in its respective cluster","title":"Linkage"},{"location":"api/linkage/#polyfuzzlinkage","text":"","title":"polyfuzz.linkage"},{"location":"api/linkage/#polyfuzz.linkage.single_linkage","text":"Show source code in polyfuzz\\linkage.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def single_linkage ( matches : pd . DataFrame , min_similarity : float = 0.8 ) -> Tuple [ Mapping [ int , List [ str ]], Mapping [ str , int ], Mapping [ str , str ]]: \"\"\" Single linkage clustering from column 'From' to column 'To' `matches` contains three columns: *From*, *To*, and *Similarity* where *Similarity* is already the minimum similarity score and thus no checking for minimum similarity is necessary. Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for creating groups min_similarity: minimum similarity between strings before they can be merged into a group Returns: clusters: The populated clusters cluster_mapping: The mapping from a string to a cluster cluster_name_map: The mapping from a string to the representative string in its respective cluster \"\"\" matches = matches . loc [ matches . Similarity > min_similarity , :] cluster_mapping = {} cluster_id = 0 for row in matches . itertuples (): # If from string has not already been mapped if not cluster_mapping . get ( row . From ): # If the to string has not already been mapped if not cluster_mapping . get ( row . To ): cluster_mapping [ row . To ] = cluster_id cluster_mapping [ row . From ] = cluster_id cluster_id += 1 # If the to string has already been mapped else : cluster_mapping [ row . From ] = cluster_mapping . get ( row . To ) # Populate the clusters clusters = {} for key , value in cluster_mapping . items (): clusters . setdefault ( value , []) clusters [ value ] . append ( key ) cluster_name_map = { key : clusters . get ( value )[ 0 ] for key , value in cluster_mapping . items ()} return clusters , cluster_mapping , cluster_name_map Single linkage clustering from column 'From' to column 'To' matches contains three columns: From , To , and Similarity where Similarity is already the minimum similarity score and thus no checking for minimum similarity is necessary. Parameters Name Type Description Default matches DataFrame contains the columns From , To , and Similarity used for creating groups required min_similarity float minimum similarity between strings before they can be merged into a group 0.8 Returns Type Description Tuple[Mapping[int, List[str]], Mapping[str, int], Mapping[str, str]] clusters: The populated clusters cluster_mapping: The mapping from a string to a cluster cluster_name_map: The mapping from a string to the representative string in its respective cluster","title":"single_linkage()"},{"location":"api/metrics/","text":"polyfuzz.metrics \u00b6 precision_recall_curve ( matches , precision_steps = 0.01 ) \u00b6 Show source code in polyfuzz\\metrics.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def precision_recall_curve ( matches : pd . DataFrame , precision_steps : float = 0.01 ) -> Tuple [ List [ float ], List [ float ], List [ float ]]: \"\"\" Calculate precision recall curve based on minimum similarity between strings A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for calculating precision, recall, and average precision precision_steps: the incremental steps in minimum precision Returns: min_precisions: minimum precision steps recall: recall per minimum precision step average_precision: average precision per minimum precision step \"\"\" min_precisions = list ( np . arange ( 0. , 1 + precision_steps , precision_steps )) average_precision = [] recall = [] similarities = matches . Similarity . values total = len ( matches ) for min_precision in min_precisions : selection = similarities [ similarities >= min_precision ] recall . append ( len ( selection ) / total ) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = RuntimeWarning ) average_precision . append ( float ( np . mean ( selection ))) return min_precisions , recall , average_precision Calculate precision recall curve based on minimum similarity between strings A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as precision since it shows you how precise we believe the matches are at a minimum. Recall can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Parameters Name Type Description Default matches DataFrame contains the columns From , To , and Similarity used for calculating precision, recall, and average precision required precision_steps float the incremental steps in minimum precision 0.01 Returns Type Description Tuple[List[float], List[float], List[float]] min_precisions: minimum precision steps recall: recall per minimum precision step average_precision: average precision per minimum precision step visualize_precision_recall ( matches , min_precisions , recall , kde = True , save_path = None ) \u00b6 Show source code in polyfuzz\\metrics.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def visualize_precision_recall ( matches : Mapping [ str , pd . DataFrame ], min_precisions : Mapping [ str , List [ float ]], recall : Mapping [ str , List [ float ]], kde : bool = True , save_path : str = None ): \"\"\" Visualize the precision recall curve for one or more models Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for calculating precision, recall, and average precision per model min_precisions: minimum precision steps per model recall: recall per minimum precision step per model kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python visualize_precision_recall(matches, min_precisions, recall, save_path=\"data/results.png\") ``` \"\"\" SMALL_SIZE = 10 MEDIUM_SIZE = 12 BIGGER_SIZE = 14 plt . rc ( 'font' , size = SMALL_SIZE ) # controls default text sizes plt . rc ( 'axes' , titlesize = SMALL_SIZE ) # fontsize of the axes title plt . rc ( 'axes' , labelsize = MEDIUM_SIZE ) # fontsize of the x and y labels plt . rc ( 'xtick' , labelsize = SMALL_SIZE ) # fontsize of the tick labels plt . rc ( 'ytick' , labelsize = SMALL_SIZE ) # fontsize of the tick labels plt . rc ( 'legend' , fontsize = SMALL_SIZE ) # legend fontsize plt . rc ( 'figure' , titlesize = BIGGER_SIZE ) # fontsize of the figure title if not isinstance ( matches , dict ): matches = { \"Model\" : matches } min_precisions = { \"Model\" : min_precisions } recall = { \"Model\" : recall } # Create single dataset of similarity score for all models distribution_data = [( matches [ name ] . Similarity . values , [ name for _ in range ( len ( matches [ name ]))]) for name in matches . keys ()] distribution_data = pd . DataFrame ( np . hstack ( distribution_data ) . T , columns = [ \"Similarity\" , \"Model\" ]) distribution_data . Similarity = distribution_data . Similarity . astype ( float ) model_names = list ( matches . keys ()) # Create layout cmap = get_cmap ( 'Accent' ) fig = plt . figure ( figsize = ( 20 , 5 )) if len ( model_names ) == 1 : middle = 0 else : middle = . 1 if kde : widths = [ 1.5 , middle , 1.5 ] else : widths = [ 1.5 , middle , 0 ] heights = [ 1.5 ] gs = gridspec . GridSpec ( 1 , 3 , width_ratios = widths , height_ratios = heights ) ax1 = plt . subplot ( gs [:, 0 ]) if kde : ax2 = plt . subplot ( gs [:, 2 ], sharex = ax1 ) # Precision-recall curve for color , model_name in zip ( cmap . colors , model_names ): ax1 . plot ( min_precisions [ model_name ], recall [ model_name ], color = color ) ax1 . set_ylim ( bottom = 0 , top = 1 ) ax1 . set_xlim ( left = 0 , right = 1 ) ax1 . spines [ 'right' ] . set_visible ( False ) ax1 . spines [ 'top' ] . set_visible ( False ) ax1 . set_xlabel ( r \"$\\bf {Precision} $\" + \" \\n (Minimum Similarity)\" ) ax1 . set_ylabel ( r \"$\\bf {Recall} $\" + \" \\n (Percentage Matched)\" ) # Similarity Histogram if kde : for color , model_name in zip ( cmap . colors , model_names ): sns . kdeplot ( matches [ model_name ][ \"Similarity\" ], fill = True , ax = ax2 , color = color ) ax2 . yaxis . set_label_position ( \"right\" ) ax2 . yaxis . tick_right () ax2 . set_xlabel ( r \"$\\bf {Similarity} $\" ) ax2 . set_ylabel ( \"\" ) ax2 . set_xlim ( left =- 0 , right = 1 ) plt . setp ([ ax2 ], title = 'Score Frequency - KDE' ) # Titles if len ( model_names ) == 1 and kde : fig . suptitle ( f 'Score Metrics' , size = 20 , y = 1 , x = 0.5 ) plt . setp ([ ax1 ], title = 'Precision-Recall Curve' ) elif kde : fig . suptitle ( 'Score Metrics' , size = 20 , y = 1 , x = 0.5 ) plt . setp ([ ax1 ], title = 'Precision-Recall Curve' ) else : fig . suptitle ( 'Precision-Recall Curve' , size = 20 , y = 1 , x = 0.45 ) # Custom Legend if len ( model_names ) > 1 : custom_lines = [ Line2D ([ 0 ], [ 0 ], color = color , lw = 4 ) for color , model_name in zip ( cmap . colors , model_names )] ax1 . legend ( custom_lines , model_names , bbox_to_anchor = ( 1.05 , . 61 , . 7 , . 902 ), loc = 3 , ncol = 1 , borderaxespad = 0. , frameon = True , fontsize = 10 ) if save_path : plt . savefig ( save_path , dpi = 300 ) Visualize the precision recall curve for one or more models Parameters Name Type Description Default matches Mapping[str, pandas.core.frame.DataFrame] contains the columns From , To , and Similarity used for calculating precision, recall, and average precision per model required min_precisions Mapping[str, List[float]] minimum precision steps per model required recall Mapping[str, List[float]] recall per minimum precision step per model required kde bool whether to also visualize the kde plot True save_path str the path to save the resulting image to None Usage: visualize_precision_recall ( matches , min_precisions , recall , save_path = \"data/results.png\" )","title":"Metrics"},{"location":"api/metrics/#polyfuzzmetrics","text":"","title":"polyfuzz.metrics"},{"location":"api/metrics/#polyfuzz.metrics.precision_recall_curve","text":"Show source code in polyfuzz\\metrics.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def precision_recall_curve ( matches : pd . DataFrame , precision_steps : float = 0.01 ) -> Tuple [ List [ float ], List [ float ], List [ float ]]: \"\"\" Calculate precision recall curve based on minimum similarity between strings A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for calculating precision, recall, and average precision precision_steps: the incremental steps in minimum precision Returns: min_precisions: minimum precision steps recall: recall per minimum precision step average_precision: average precision per minimum precision step \"\"\" min_precisions = list ( np . arange ( 0. , 1 + precision_steps , precision_steps )) average_precision = [] recall = [] similarities = matches . Similarity . values total = len ( matches ) for min_precision in min_precisions : selection = similarities [ similarities >= min_precision ] recall . append ( len ( selection ) / total ) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = RuntimeWarning ) average_precision . append ( float ( np . mean ( selection ))) return min_precisions , recall , average_precision Calculate precision recall curve based on minimum similarity between strings A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as precision since it shows you how precise we believe the matches are at a minimum. Recall can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Parameters Name Type Description Default matches DataFrame contains the columns From , To , and Similarity used for calculating precision, recall, and average precision required precision_steps float the incremental steps in minimum precision 0.01 Returns Type Description Tuple[List[float], List[float], List[float]] min_precisions: minimum precision steps recall: recall per minimum precision step average_precision: average precision per minimum precision step","title":"precision_recall_curve()"},{"location":"api/metrics/#polyfuzz.metrics.visualize_precision_recall","text":"Show source code in polyfuzz\\metrics.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def visualize_precision_recall ( matches : Mapping [ str , pd . DataFrame ], min_precisions : Mapping [ str , List [ float ]], recall : Mapping [ str , List [ float ]], kde : bool = True , save_path : str = None ): \"\"\" Visualize the precision recall curve for one or more models Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for calculating precision, recall, and average precision per model min_precisions: minimum precision steps per model recall: recall per minimum precision step per model kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python visualize_precision_recall(matches, min_precisions, recall, save_path=\"data/results.png\") ``` \"\"\" SMALL_SIZE = 10 MEDIUM_SIZE = 12 BIGGER_SIZE = 14 plt . rc ( 'font' , size = SMALL_SIZE ) # controls default text sizes plt . rc ( 'axes' , titlesize = SMALL_SIZE ) # fontsize of the axes title plt . rc ( 'axes' , labelsize = MEDIUM_SIZE ) # fontsize of the x and y labels plt . rc ( 'xtick' , labelsize = SMALL_SIZE ) # fontsize of the tick labels plt . rc ( 'ytick' , labelsize = SMALL_SIZE ) # fontsize of the tick labels plt . rc ( 'legend' , fontsize = SMALL_SIZE ) # legend fontsize plt . rc ( 'figure' , titlesize = BIGGER_SIZE ) # fontsize of the figure title if not isinstance ( matches , dict ): matches = { \"Model\" : matches } min_precisions = { \"Model\" : min_precisions } recall = { \"Model\" : recall } # Create single dataset of similarity score for all models distribution_data = [( matches [ name ] . Similarity . values , [ name for _ in range ( len ( matches [ name ]))]) for name in matches . keys ()] distribution_data = pd . DataFrame ( np . hstack ( distribution_data ) . T , columns = [ \"Similarity\" , \"Model\" ]) distribution_data . Similarity = distribution_data . Similarity . astype ( float ) model_names = list ( matches . keys ()) # Create layout cmap = get_cmap ( 'Accent' ) fig = plt . figure ( figsize = ( 20 , 5 )) if len ( model_names ) == 1 : middle = 0 else : middle = . 1 if kde : widths = [ 1.5 , middle , 1.5 ] else : widths = [ 1.5 , middle , 0 ] heights = [ 1.5 ] gs = gridspec . GridSpec ( 1 , 3 , width_ratios = widths , height_ratios = heights ) ax1 = plt . subplot ( gs [:, 0 ]) if kde : ax2 = plt . subplot ( gs [:, 2 ], sharex = ax1 ) # Precision-recall curve for color , model_name in zip ( cmap . colors , model_names ): ax1 . plot ( min_precisions [ model_name ], recall [ model_name ], color = color ) ax1 . set_ylim ( bottom = 0 , top = 1 ) ax1 . set_xlim ( left = 0 , right = 1 ) ax1 . spines [ 'right' ] . set_visible ( False ) ax1 . spines [ 'top' ] . set_visible ( False ) ax1 . set_xlabel ( r \"$\\bf {Precision} $\" + \" \\n (Minimum Similarity)\" ) ax1 . set_ylabel ( r \"$\\bf {Recall} $\" + \" \\n (Percentage Matched)\" ) # Similarity Histogram if kde : for color , model_name in zip ( cmap . colors , model_names ): sns . kdeplot ( matches [ model_name ][ \"Similarity\" ], fill = True , ax = ax2 , color = color ) ax2 . yaxis . set_label_position ( \"right\" ) ax2 . yaxis . tick_right () ax2 . set_xlabel ( r \"$\\bf {Similarity} $\" ) ax2 . set_ylabel ( \"\" ) ax2 . set_xlim ( left =- 0 , right = 1 ) plt . setp ([ ax2 ], title = 'Score Frequency - KDE' ) # Titles if len ( model_names ) == 1 and kde : fig . suptitle ( f 'Score Metrics' , size = 20 , y = 1 , x = 0.5 ) plt . setp ([ ax1 ], title = 'Precision-Recall Curve' ) elif kde : fig . suptitle ( 'Score Metrics' , size = 20 , y = 1 , x = 0.5 ) plt . setp ([ ax1 ], title = 'Precision-Recall Curve' ) else : fig . suptitle ( 'Precision-Recall Curve' , size = 20 , y = 1 , x = 0.45 ) # Custom Legend if len ( model_names ) > 1 : custom_lines = [ Line2D ([ 0 ], [ 0 ], color = color , lw = 4 ) for color , model_name in zip ( cmap . colors , model_names )] ax1 . legend ( custom_lines , model_names , bbox_to_anchor = ( 1.05 , . 61 , . 7 , . 902 ), loc = 3 , ncol = 1 , borderaxespad = 0. , frameon = True , fontsize = 10 ) if save_path : plt . savefig ( save_path , dpi = 300 ) Visualize the precision recall curve for one or more models Parameters Name Type Description Default matches Mapping[str, pandas.core.frame.DataFrame] contains the columns From , To , and Similarity used for calculating precision, recall, and average precision per model required min_precisions Mapping[str, List[float]] minimum precision steps per model required recall Mapping[str, List[float]] recall per minimum precision step per model required kde bool whether to also visualize the kde plot True save_path str the path to save the resulting image to None Usage: visualize_precision_recall ( matches , min_precisions , recall , save_path = \"data/results.png\" )","title":"visualize_precision_recall()"},{"location":"api/polyfuzz/","text":"polyfuzz.polyfuzz.PolyFuzz \u00b6 PolyFuzz class for Fuzzy string matching, grouping, and evaluation. Parameters Name Type Description Default method Union[str, polyfuzz.models._base.BaseMatcher, List[polyfuzz.models._base.BaseMatcher]] the method(s) used for matching. For quick selection of models select one of the following: \"EditDistance\", \"TF-IDF\" or \"Embeddings\". If you want more control over the models above, pass in a model from polyfuzz.models. For examples, see usage below. 'TF-IDF' verbose bool Changes the verbosity of the model, Set to True if you want to track the stages of the model. False Usage: For basic, out-of-the-box usage, run the code below. You can replace \"TF-IDF\" with either \"EditDistance\" or \"Embeddings\" for quick access to these models: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" ) If you want more control over the String Matching models, you can load in these models separately: tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , model_id = \"TF-IDF-Sklearn\" ) model = pf . PolyFuzz ( tfidf ) You can also select multiple models in order to compare performance: tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , model_id = \"TF-IDF-Sklearn\" ) edit = EditDistance ( n_jobs =- 1 ) model = pf . PolyFuzz ([ tfidf , edit ]) To use embedding models, please use Flair word embeddings: from flair.embeddings import WordEmbeddings , TransformerWordEmbeddings fasttext_embedding = WordEmbeddings ( 'news' ) bert_embedding = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) embedding = Embeddings ([ fasttext_embedding , bert_embedding ], min_similarity = 0.0 ) model = pf . PolyFuzz ( embedding ) get_cluster_mappings ( self , name = None ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 275 276 277 278 279 280 281 282 283 284 285 286 def get_cluster_mappings ( self , name : str = None ) -> Mapping [ str , int ]: \"\"\" Get the mappings from the `To` column to its respective column \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . cluster_mappings . values ())[ 0 ] elif len ( self . matches ) > 1 and name : return self . cluster_mappings [ name ] return self . cluster_mappings Get the mappings from the To column to its respective column get_clusters ( self , model_id = None ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def get_clusters ( self , model_id : str = None ) -> Mapping [ str , List [ str ]]: \"\"\" Get the groupings/clusters from a single model Arguments: model_id: the model id of the model if you have specified multiple models \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . clusters . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . clusters [ model_id ] return self . clusters Get the groupings/clusters from a single model Parameters Name Type Description Default model_id str the model id of the model if you have specified multiple models None get_ids ( self ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 234 235 236 237 238 239 240 241 242 def get_ids ( self ) -> Union [ str , List [ str ], None ]: \"\"\" Get all model ids for easier access \"\"\" check_matches ( self ) if isinstance ( self . method , str ): return self . method elif isinstance ( self . method , Iterable ): return [ model . model_id for model in self . method ] return None Get all model ids for easier access get_matches ( self , model_id = None ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 244 245 246 247 248 249 250 251 252 253 254 255 def get_matches ( self , model_id : str = None ) -> Union [ pd . DataFrame , Mapping [ str , pd . DataFrame ]]: \"\"\" Get the matches from one or more models\"\"\" check_matches ( self ) if len ( self . matches ) == 1 : return list ( self . matches . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . matches [ model_id ] return self . matches Get the matches from one or more models group ( self , model = None , link_min_similarity = 0.75 , group_all_strings = False ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def group ( self , model : Union [ str , BaseMatcher ] = None , link_min_similarity : float = 0.75 , group_all_strings : bool = False ): \"\"\" From the matches, group the `To` matches together using single linkage Arguments: model: you can choose one of the models in `polyfuzz.models` to be used as a grouper link_min_similarity: the minimum similarity between strings before they are grouped in a single linkage fashion group_all_strings: if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. Updates: self.matches: Adds a column `Group` that is the grouped version of the `To` column \"\"\" check_matches ( self ) self . clusters = {} self . cluster_mappings = {} # Standard models - quick access if isinstance ( model , str ): if model in [ \"TF-IDF\" , \"TFIDF\" ]: model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: model = RapidFuzz () elif self . method in [ \"Embeddings\" , \"Embedding\" ]: model = Embeddings ( min_similarity = link_min_similarity ) else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" \"* Or None if you want to automatically use TF-IDF\" ) # Use TF-IDF if no model is specified elif not model : model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) # Group per model for name , match in self . matches . items (): self . _create_groups ( name , model , link_min_similarity , group_all_strings ) From the matches, group the To matches together using single linkage Parameters Name Type Description Default model Union[str, polyfuzz.models._base.BaseMatcher] you can choose one of the models in polyfuzz.models to be used as a grouper None link_min_similarity float the minimum similarity between strings before they are grouped in a single linkage fashion 0.75 group_all_strings bool if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. False Updates: self.matches: Adds a column Group that is the grouped version of the To column match ( self , from_list , to_list ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def match ( self , from_list : List [ str ], to_list : List [ str ]): \"\"\" Match the from_list of strings to the to_list of strings with whatever models you have initialized Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Updates: self.matches: A dictionary with the matches from all models, can be accessed with `model.get_all_matches` or `model.get_match(\"TF-IDF\")` Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` You can access the results matches with `model.get_all_matches` or a specific model with `model.get_match(\"TF-IDF\")` based on their model_id. \"\"\" # Standard models - quick access if isinstance ( self . method , str ): if self . method in [ \"TF-IDF\" , \"TFIDF\" ]: self . matches = { \"TF-IDF\" : TFIDF ( min_similarity = 0 ) . match ( from_list , to_list )} elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: self . matches = { \"EditDistance\" : RapidFuzz () . match ( from_list , to_list )} elif self . method in [ \"Embeddings\" , \"Embedding\" ]: self . matches = { \"Embeddings\" : Embeddings ( min_similarity = 0 ) . match ( from_list , to_list )} else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" ) logger . info ( f \"Ran model with model id = {self.method} \" ) # Custom models elif isinstance ( self . method , BaseMatcher ): self . matches = { self . method . model_id : self . method . match ( from_list , to_list )} logging . info ( f \"Ran model with model id = {self.method.model_id} \" ) # Multiple custom models elif isinstance ( self . method , Iterable ): self . _update_model_ids () self . matches = {} for model in self . method : self . matches [ model . model_id ] = model . match ( from_list , to_list ) logging . info ( f \"Ran model with model id = {model.model_id} \" ) return self Match the from_list of strings to the to_list of strings with whatever models you have initialized Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Updates: self.matches: A dictionary with the matches from all models, can be accessed with model.get_all_matches or model.get_match(\"TF-IDF\") Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . match ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) You can access the results matches with model.get_all_matches or a specific model with model.get_match(\"TF-IDF\") based on their model_id. visualize_precision_recall ( self , kde = False , save_path = None ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def visualize_precision_recall ( self , kde : bool = False , save_path : str = None ): \"\"\" Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) model.visualize_precision_recall(save_path=\"results.png\") ``` \"\"\" check_matches ( self ) self . min_precisions = {} self . recalls = {} self . average_precisions = {} for name , match in self . matches . items (): min_precision , recall , average_precision = precision_recall_curve ( match ) self . min_precisions [ name ] = min_precision self . recalls [ name ] = recall self . average_precisions [ name ] = average_precision visualize_precision_recall ( self . matches , self . min_precisions , self . recalls , kde , save_path ) Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as precision since it shows you how precise we believe the matches are at a minimum. Recall can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Parameters Name Type Description Default kde bool whether to also visualize the kde plot False save_path str the path to save the resulting image to None Usage: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . match ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) model . visualize_precision_recall ( save_path = \"results.png\" )","title":"PolyFuzz"},{"location":"api/polyfuzz/#polyfuzzpolyfuzzpolyfuzz","text":"PolyFuzz class for Fuzzy string matching, grouping, and evaluation. Parameters Name Type Description Default method Union[str, polyfuzz.models._base.BaseMatcher, List[polyfuzz.models._base.BaseMatcher]] the method(s) used for matching. For quick selection of models select one of the following: \"EditDistance\", \"TF-IDF\" or \"Embeddings\". If you want more control over the models above, pass in a model from polyfuzz.models. For examples, see usage below. 'TF-IDF' verbose bool Changes the verbosity of the model, Set to True if you want to track the stages of the model. False Usage: For basic, out-of-the-box usage, run the code below. You can replace \"TF-IDF\" with either \"EditDistance\" or \"Embeddings\" for quick access to these models: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" ) If you want more control over the String Matching models, you can load in these models separately: tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , model_id = \"TF-IDF-Sklearn\" ) model = pf . PolyFuzz ( tfidf ) You can also select multiple models in order to compare performance: tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , model_id = \"TF-IDF-Sklearn\" ) edit = EditDistance ( n_jobs =- 1 ) model = pf . PolyFuzz ([ tfidf , edit ]) To use embedding models, please use Flair word embeddings: from flair.embeddings import WordEmbeddings , TransformerWordEmbeddings fasttext_embedding = WordEmbeddings ( 'news' ) bert_embedding = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) embedding = Embeddings ([ fasttext_embedding , bert_embedding ], min_similarity = 0.0 ) model = pf . PolyFuzz ( embedding )","title":"polyfuzz.polyfuzz.PolyFuzz"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.get_cluster_mappings","text":"Show source code in polyfuzz\\polyfuzz.py 275 276 277 278 279 280 281 282 283 284 285 286 def get_cluster_mappings ( self , name : str = None ) -> Mapping [ str , int ]: \"\"\" Get the mappings from the `To` column to its respective column \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . cluster_mappings . values ())[ 0 ] elif len ( self . matches ) > 1 and name : return self . cluster_mappings [ name ] return self . cluster_mappings Get the mappings from the To column to its respective column","title":"get_cluster_mappings()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.get_clusters","text":"Show source code in polyfuzz\\polyfuzz.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def get_clusters ( self , model_id : str = None ) -> Mapping [ str , List [ str ]]: \"\"\" Get the groupings/clusters from a single model Arguments: model_id: the model id of the model if you have specified multiple models \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . clusters . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . clusters [ model_id ] return self . clusters Get the groupings/clusters from a single model Parameters Name Type Description Default model_id str the model id of the model if you have specified multiple models None","title":"get_clusters()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.get_ids","text":"Show source code in polyfuzz\\polyfuzz.py 234 235 236 237 238 239 240 241 242 def get_ids ( self ) -> Union [ str , List [ str ], None ]: \"\"\" Get all model ids for easier access \"\"\" check_matches ( self ) if isinstance ( self . method , str ): return self . method elif isinstance ( self . method , Iterable ): return [ model . model_id for model in self . method ] return None Get all model ids for easier access","title":"get_ids()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.get_matches","text":"Show source code in polyfuzz\\polyfuzz.py 244 245 246 247 248 249 250 251 252 253 254 255 def get_matches ( self , model_id : str = None ) -> Union [ pd . DataFrame , Mapping [ str , pd . DataFrame ]]: \"\"\" Get the matches from one or more models\"\"\" check_matches ( self ) if len ( self . matches ) == 1 : return list ( self . matches . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . matches [ model_id ] return self . matches Get the matches from one or more models","title":"get_matches()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.group","text":"Show source code in polyfuzz\\polyfuzz.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def group ( self , model : Union [ str , BaseMatcher ] = None , link_min_similarity : float = 0.75 , group_all_strings : bool = False ): \"\"\" From the matches, group the `To` matches together using single linkage Arguments: model: you can choose one of the models in `polyfuzz.models` to be used as a grouper link_min_similarity: the minimum similarity between strings before they are grouped in a single linkage fashion group_all_strings: if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. Updates: self.matches: Adds a column `Group` that is the grouped version of the `To` column \"\"\" check_matches ( self ) self . clusters = {} self . cluster_mappings = {} # Standard models - quick access if isinstance ( model , str ): if model in [ \"TF-IDF\" , \"TFIDF\" ]: model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: model = RapidFuzz () elif self . method in [ \"Embeddings\" , \"Embedding\" ]: model = Embeddings ( min_similarity = link_min_similarity ) else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" \"* Or None if you want to automatically use TF-IDF\" ) # Use TF-IDF if no model is specified elif not model : model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) # Group per model for name , match in self . matches . items (): self . _create_groups ( name , model , link_min_similarity , group_all_strings ) From the matches, group the To matches together using single linkage Parameters Name Type Description Default model Union[str, polyfuzz.models._base.BaseMatcher] you can choose one of the models in polyfuzz.models to be used as a grouper None link_min_similarity float the minimum similarity between strings before they are grouped in a single linkage fashion 0.75 group_all_strings bool if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. False Updates: self.matches: Adds a column Group that is the grouped version of the To column","title":"group()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.match","text":"Show source code in polyfuzz\\polyfuzz.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def match ( self , from_list : List [ str ], to_list : List [ str ]): \"\"\" Match the from_list of strings to the to_list of strings with whatever models you have initialized Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Updates: self.matches: A dictionary with the matches from all models, can be accessed with `model.get_all_matches` or `model.get_match(\"TF-IDF\")` Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` You can access the results matches with `model.get_all_matches` or a specific model with `model.get_match(\"TF-IDF\")` based on their model_id. \"\"\" # Standard models - quick access if isinstance ( self . method , str ): if self . method in [ \"TF-IDF\" , \"TFIDF\" ]: self . matches = { \"TF-IDF\" : TFIDF ( min_similarity = 0 ) . match ( from_list , to_list )} elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: self . matches = { \"EditDistance\" : RapidFuzz () . match ( from_list , to_list )} elif self . method in [ \"Embeddings\" , \"Embedding\" ]: self . matches = { \"Embeddings\" : Embeddings ( min_similarity = 0 ) . match ( from_list , to_list )} else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" ) logger . info ( f \"Ran model with model id = {self.method} \" ) # Custom models elif isinstance ( self . method , BaseMatcher ): self . matches = { self . method . model_id : self . method . match ( from_list , to_list )} logging . info ( f \"Ran model with model id = {self.method.model_id} \" ) # Multiple custom models elif isinstance ( self . method , Iterable ): self . _update_model_ids () self . matches = {} for model in self . method : self . matches [ model . model_id ] = model . match ( from_list , to_list ) logging . info ( f \"Ran model with model id = {model.model_id} \" ) return self Match the from_list of strings to the to_list of strings with whatever models you have initialized Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Updates: self.matches: A dictionary with the matches from all models, can be accessed with model.get_all_matches or model.get_match(\"TF-IDF\") Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . match ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) You can access the results matches with model.get_all_matches or a specific model with model.get_match(\"TF-IDF\") based on their model_id.","title":"match()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.visualize_precision_recall","text":"Show source code in polyfuzz\\polyfuzz.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def visualize_precision_recall ( self , kde : bool = False , save_path : str = None ): \"\"\" Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) model.visualize_precision_recall(save_path=\"results.png\") ``` \"\"\" check_matches ( self ) self . min_precisions = {} self . recalls = {} self . average_precisions = {} for name , match in self . matches . items (): min_precision , recall , average_precision = precision_recall_curve ( match ) self . min_precisions [ name ] = min_precision self . recalls [ name ] = recall self . average_precisions [ name ] = average_precision visualize_precision_recall ( self . matches , self . min_precisions , self . recalls , kde , save_path ) Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as precision since it shows you how precise we believe the matches are at a minimum. Recall can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Parameters Name Type Description Default kde bool whether to also visualize the kde plot False save_path str the path to save the resulting image to None Usage: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . match ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) model . visualize_precision_recall ( save_path = \"results.png\" )","title":"visualize_precision_recall()"},{"location":"api/models/base/","text":"polyfuzz.models.BaseMatcher \u00b6 The abstract BaseMatching to be modelled after for string matching match ( self , from_list , to_list ) \u00b6 Show source code in models\\_base.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @abstractmethod def match ( self , from_list : List [ str ], to_list : List [ str ]) -> pd . DataFrame : \"\"\" Make sure you follow the same argument structure: Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\" \"\"\" raise NotImplementedError () Make sure you follow the same argument structure: Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Returns Type Description DataFrame matches: The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\"","title":"BaseMatcher"},{"location":"api/models/base/#polyfuzzmodelsbasematcher","text":"The abstract BaseMatching to be modelled after for string matching","title":"polyfuzz.models.BaseMatcher"},{"location":"api/models/base/#polyfuzz.models._base.BaseMatcher.match","text":"Show source code in models\\_base.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @abstractmethod def match ( self , from_list : List [ str ], to_list : List [ str ]) -> pd . DataFrame : \"\"\" Make sure you follow the same argument structure: Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\" \"\"\" raise NotImplementedError () Make sure you follow the same argument structure: Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Returns Type Description DataFrame matches: The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\"","title":"match()"},{"location":"api/models/distance/","text":"polyfuzz.models.EditDistance \u00b6 Calculate the Edit Distance between lists of strings using any distance/similarity based scorer Parameters Name Type Description Default n_jobs int Nr of parallel processes, use -1 to use all cores 1 scorer Callable The scorer function to be used to calculate the edit distance. This function should give back a float between 0 and 1, and work as follows: scorer(\"string_one\", \"string_two\") <built-in function ratio> model_id str The name of the particular instance, used when comparing models None Usage: from rapidfuzz import fuzz model = EditDistance ( n_jobs =- 1 , scorer = fuzz . WRatio ) match ( self , from_list , to_list ) \u00b6 Show source code in models\\_distance.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def match ( self , from_list : List [ str ], to_list : List [ str ]) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = EditDistance(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if from_list == to_list : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) if self . normalize : matches [ \"Similarity\" ] = ( matches [ \"Similarity\" ] - matches [ \"Similarity\" ] . min ()) / ( matches [ \"Similarity\" ] . max () - matches [ \"Similarity\" ] . min ()) return matches Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: from rapidfuzz import fuzz model = EditDistance ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"EditDistance"},{"location":"api/models/distance/#polyfuzzmodelseditdistance","text":"Calculate the Edit Distance between lists of strings using any distance/similarity based scorer Parameters Name Type Description Default n_jobs int Nr of parallel processes, use -1 to use all cores 1 scorer Callable The scorer function to be used to calculate the edit distance. This function should give back a float between 0 and 1, and work as follows: scorer(\"string_one\", \"string_two\") <built-in function ratio> model_id str The name of the particular instance, used when comparing models None Usage: from rapidfuzz import fuzz model = EditDistance ( n_jobs =- 1 , scorer = fuzz . WRatio )","title":"polyfuzz.models.EditDistance"},{"location":"api/models/distance/#polyfuzz.models._distance.EditDistance.match","text":"Show source code in models\\_distance.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def match ( self , from_list : List [ str ], to_list : List [ str ]) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = EditDistance(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if from_list == to_list : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) if self . normalize : matches [ \"Similarity\" ] = ( matches [ \"Similarity\" ] - matches [ \"Similarity\" ] . min ()) / ( matches [ \"Similarity\" ] . max () - matches [ \"Similarity\" ] . min ()) return matches Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: from rapidfuzz import fuzz model = EditDistance ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"match()"},{"location":"api/models/embeddings/","text":"polyfuzz.models.Embeddings \u00b6 Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Parameters Name Type Description Default embedding_method Optional[List] list of Flair embeddings to use None min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 cosine_method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' model_id str The name of the particular instance, used when comparing models None Usage: model = Embeddings ( min_similarity = 0.5 ) Or if you want a custom model to be used and it is a word embedding model, pass it in as a list: embedding_model = WordEmbeddings ( 'news' ) model = Embeddings ([ embeddings_model ], min_similarity = 0.5 ) As you might have guessed, you can pass along multiple word embedding models and the results will be averaged: fasttext_embedding = WordEmbeddings ( 'news' ) glove_embedding = WordEmbeddings ( 'glove' ) bert_embedding = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) model = Embeddings ([ glove_embedding , fasttext_embedding , bert_embedding ], min_similarity = 0.5 ) match ( self , from_list , to_list , embeddings_from = None , embeddings_to = None ) \u00b6 Show source code in models\\_embeddings.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def match ( self , from_list : List [ str ], to_list : List [ str ], embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if not isinstance ( embeddings_from , np . ndarray ): embeddings_from = self . _embed ( from_list ) if not isinstance ( embeddings_to , np . ndarray ): embeddings_to = self . _embed ( to_list ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , self . cosine_method ) return matches Matches the two lists of strings to each other and returns the best mapping Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required embeddings_from ndarray Embeddings you created yourself from the from_list None embeddings_to ndarray Embeddings you created yourself from the to_list None Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: model = Embeddings ( min_similarity = 0.5 ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"Embeddings"},{"location":"api/models/embeddings/#polyfuzzmodelsembeddings","text":"Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Parameters Name Type Description Default embedding_method Optional[List] list of Flair embeddings to use None min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 cosine_method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' model_id str The name of the particular instance, used when comparing models None Usage: model = Embeddings ( min_similarity = 0.5 ) Or if you want a custom model to be used and it is a word embedding model, pass it in as a list: embedding_model = WordEmbeddings ( 'news' ) model = Embeddings ([ embeddings_model ], min_similarity = 0.5 ) As you might have guessed, you can pass along multiple word embedding models and the results will be averaged: fasttext_embedding = WordEmbeddings ( 'news' ) glove_embedding = WordEmbeddings ( 'glove' ) bert_embedding = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) model = Embeddings ([ glove_embedding , fasttext_embedding , bert_embedding ], min_similarity = 0.5 )","title":"polyfuzz.models.Embeddings"},{"location":"api/models/embeddings/#polyfuzz.models._embeddings.Embeddings.match","text":"Show source code in models\\_embeddings.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def match ( self , from_list : List [ str ], to_list : List [ str ], embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if not isinstance ( embeddings_from , np . ndarray ): embeddings_from = self . _embed ( from_list ) if not isinstance ( embeddings_to , np . ndarray ): embeddings_to = self . _embed ( to_list ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , self . cosine_method ) return matches Matches the two lists of strings to each other and returns the best mapping Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required embeddings_from ndarray Embeddings you created yourself from the from_list None embeddings_to ndarray Embeddings you created yourself from the to_list None Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: model = Embeddings ( min_similarity = 0.5 ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"match()"},{"location":"api/models/matches/","text":"polyfuzz.models.cosine_similarity \u00b6 Calculate similarity between two matrices/vectors and return best matches Parameters Name Type Description Default from_vector ndarray the matrix or vector representing the embedded strings to map from required to_vector ndarray the matrix or vector representing the embedded strings to map to required from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: Make sure to fill the to_vector and from_vector with vector representations of to_list and from_list respectively: from polyfuzz.models import extract_best_matches indices , similarity = extract_best_matches ( from_vector , to_vector , method = \"sparse\" )","title":"CosineSimilarity"},{"location":"api/models/matches/#polyfuzzmodelscosine_similarity","text":"Calculate similarity between two matrices/vectors and return best matches Parameters Name Type Description Default from_vector ndarray the matrix or vector representing the embedded strings to map from required to_vector ndarray the matrix or vector representing the embedded strings to map to required from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: Make sure to fill the to_vector and from_vector with vector representations of to_list and from_list respectively: from polyfuzz.models import extract_best_matches indices , similarity = extract_best_matches ( from_vector , to_vector , method = \"sparse\" )","title":"polyfuzz.models.cosine_similarity"},{"location":"api/models/rapidfuzz/","text":"polyfuzz.models.RapidFuzz \u00b6 Calculate the Edit Distance between lists of strings using RapidFuzz's process function We are using RapidFuzz instead of FuzzyWuzzy since it is much faster and does not require the more restrictive GPL license Parameters Name Type Description Default n_jobs int Nr of parallel processes, use -1 to use all cores 1 score_cutoff float The minimum similarity for which to return a good match. Should be between 0 and 1. 0 scorer Callable The scorer function to be used to calculate the edit distance Options: * fuzz.ratio * fuzz.partial_ratio * fuzz.token_sort_ratio * fuzz.partial_token_sort_ratio * fuzz.token_set_ratio * fuzz.partial_token_set_ratio * fuzz.token_ratio * fuzz.partial_token_ratio * fuzz.WRation * fuzz.QRatio See https://maxbachmann.github.io/rapidfuzz/usage/fuzz/ for an extensive description of the scoring methods. <built-in function WRatio> model_id str The name of the particular instance, used when comparing models None Usage: from rapidfuzz import fuzz model = RapidFuzz ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio ) match ( self , from_list , to_list ) \u00b6 Show source code in models\\_rapidfuzz.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def match ( self , from_list : List [ str ], to_list : List [ str ]) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = RapidFuzz(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if from_list == to_list : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) return matches Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: from rapidfuzz import fuzz model = RapidFuzz ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"RapidFuzzy"},{"location":"api/models/rapidfuzz/#polyfuzzmodelsrapidfuzz","text":"Calculate the Edit Distance between lists of strings using RapidFuzz's process function We are using RapidFuzz instead of FuzzyWuzzy since it is much faster and does not require the more restrictive GPL license Parameters Name Type Description Default n_jobs int Nr of parallel processes, use -1 to use all cores 1 score_cutoff float The minimum similarity for which to return a good match. Should be between 0 and 1. 0 scorer Callable The scorer function to be used to calculate the edit distance Options: * fuzz.ratio * fuzz.partial_ratio * fuzz.token_sort_ratio * fuzz.partial_token_sort_ratio * fuzz.token_set_ratio * fuzz.partial_token_set_ratio * fuzz.token_ratio * fuzz.partial_token_ratio * fuzz.WRation * fuzz.QRatio See https://maxbachmann.github.io/rapidfuzz/usage/fuzz/ for an extensive description of the scoring methods. <built-in function WRatio> model_id str The name of the particular instance, used when comparing models None Usage: from rapidfuzz import fuzz model = RapidFuzz ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio )","title":"polyfuzz.models.RapidFuzz"},{"location":"api/models/rapidfuzz/#polyfuzz.models._rapidfuzz.RapidFuzz.match","text":"Show source code in models\\_rapidfuzz.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def match ( self , from_list : List [ str ], to_list : List [ str ]) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = RapidFuzz(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if from_list == to_list : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) return matches Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: from rapidfuzz import fuzz model = RapidFuzz ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"match()"},{"location":"api/models/tfidf/","text":"polyfuzz.models.TFIDF \u00b6 A character based n-gram TF-IDF to approximate edit distance We turn a string into, typically of length 3, n-grams. For example, using 3-grams of the \"hotel\" we get ['hot', 'ote', 'tel']. These are then used as input for a TfidfVectorizer in order to create a vector for each word. Then, we simply apply cosine similarity through k-NN Parameters Name Type Description Default n_gram_range Tuple[int, int] The n_gram_range on a character-level (3, 3) clean_string bool Whether to clean the string such that only alphanumerical characters are kept True min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 cosine_method str The method/package for calculating the cosine similarity. Options: * sparse * sklearn * knn 'sparse' sparse is the fastest and most memory efficient but requires a package that might be difficult to install sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory model_id: The name of the particular instance, used when comparing models Usage: from polymatcher.models import TFIDF model = TFIDF ( n_gram_range = ( 3 , 3 ), clean_string = True , use_knn = False ) match ( self , from_list , to_list ) \u00b6 Show source code in models\\_tfidf.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def match ( self , from_list : List [ str ], to_list : List [ str ]) -> pd . DataFrame : \"\"\" Match two lists of strings to each other and return the most similar strings Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from polymatcher.models import TFIDF model = TFIDF() matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" tf_idf_from , tf_idf_to = self . _extract_tf_idf ( from_list , to_list ) matches = cosine_similarity ( tf_idf_from , tf_idf_to , from_list , to_list , self . min_similarity , self . cosine_method ) return matches Match two lists of strings to each other and return the most similar strings Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: from polymatcher.models import TFIDF model = TFIDF () matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"TFIDF"},{"location":"api/models/tfidf/#polyfuzzmodelstfidf","text":"A character based n-gram TF-IDF to approximate edit distance We turn a string into, typically of length 3, n-grams. For example, using 3-grams of the \"hotel\" we get ['hot', 'ote', 'tel']. These are then used as input for a TfidfVectorizer in order to create a vector for each word. Then, we simply apply cosine similarity through k-NN Parameters Name Type Description Default n_gram_range Tuple[int, int] The n_gram_range on a character-level (3, 3) clean_string bool Whether to clean the string such that only alphanumerical characters are kept True min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 cosine_method str The method/package for calculating the cosine similarity. Options: * sparse * sklearn * knn 'sparse' sparse is the fastest and most memory efficient but requires a package that might be difficult to install sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory model_id: The name of the particular instance, used when comparing models Usage: from polymatcher.models import TFIDF model = TFIDF ( n_gram_range = ( 3 , 3 ), clean_string = True , use_knn = False )","title":"polyfuzz.models.TFIDF"},{"location":"api/models/tfidf/#polyfuzz.models._tfidf.TFIDF.match","text":"Show source code in models\\_tfidf.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def match ( self , from_list : List [ str ], to_list : List [ str ]) -> pd . DataFrame : \"\"\" Match two lists of strings to each other and return the most similar strings Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from polymatcher.models import TFIDF model = TFIDF() matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" tf_idf_from , tf_idf_to = self . _extract_tf_idf ( from_list , to_list ) matches = cosine_similarity ( tf_idf_from , tf_idf_to , from_list , to_list , self . min_similarity , self . cosine_method ) return matches Match two lists of strings to each other and return the most similar strings Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: from polymatcher.models import TFIDF model = TFIDF () matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"match()"},{"location":"tutorial/basematcher/basematcher/","text":"Custom Models \u00b6 Although PolyFuzz has several models implemented, what if you have developed your own? What if you want a different similarity/distance measure that is not defined in PolyFuzz? That is where custom models come in. If you follow the structure of PolyFuzz's BaseMatcher you can quickly implement any model you would like. You simply create a class using BaseMatcher , make sure it has a function match that inputs two lists and outputs a pandas dataframe. That's it! We start by creating our own model that implements the ratio similarity measure from RapidFuzz: import numpy as np import pandas as pd from rapidfuzz import fuzz from polyfuzz import PolyFuzz from polyfuzz.models import BaseMatcher class MyModel ( BaseMatcher ): def match ( self , from_list , to_list ): # Calculate distances matches = [[ fuzz . ratio ( from_string , to_string ) / 100 for to_string in to_list ] for from_string in from_list ] # Get best matches mappings = [ to_list [ index ] for index in np . argmax ( matches , axis = 1 )] scores = np . max ( matches , axis = 1 ) # Prepare dataframe matches = pd . DataFrame ({ 'From' : from_list , 'To' : mappings , 'Similarity' : scores }) return matches MyModel can now be used within PolyFuzz and runs like every other model: from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] custom_matcher = MyModel () model = PolyFuzz ( custom_matcher ) . match ( from_list , to_list ) Now we can visualize the results: model . visualize_precision_recall ( kde = True )","title":"Custom Models"},{"location":"tutorial/basematcher/basematcher/#custom-models","text":"Although PolyFuzz has several models implemented, what if you have developed your own? What if you want a different similarity/distance measure that is not defined in PolyFuzz? That is where custom models come in. If you follow the structure of PolyFuzz's BaseMatcher you can quickly implement any model you would like. You simply create a class using BaseMatcher , make sure it has a function match that inputs two lists and outputs a pandas dataframe. That's it! We start by creating our own model that implements the ratio similarity measure from RapidFuzz: import numpy as np import pandas as pd from rapidfuzz import fuzz from polyfuzz import PolyFuzz from polyfuzz.models import BaseMatcher class MyModel ( BaseMatcher ): def match ( self , from_list , to_list ): # Calculate distances matches = [[ fuzz . ratio ( from_string , to_string ) / 100 for to_string in to_list ] for from_string in from_list ] # Get best matches mappings = [ to_list [ index ] for index in np . argmax ( matches , axis = 1 )] scores = np . max ( matches , axis = 1 ) # Prepare dataframe matches = pd . DataFrame ({ 'From' : from_list , 'To' : mappings , 'Similarity' : scores }) return matches MyModel can now be used within PolyFuzz and runs like every other model: from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] custom_matcher = MyModel () model = PolyFuzz ( custom_matcher ) . match ( from_list , to_list ) Now we can visualize the results: model . visualize_precision_recall ( kde = True )","title":"Custom Models"},{"location":"tutorial/datasets/datasets/","text":"Datasets \u00b6 There are two datasets prepared for you to play around with: * Company Names * Movie Titles Movie Titles \u00b6 This data is retrieved from: https://www.kaggle.com/stefanoleone992/imdb-extensive-dataset https://www.kaggle.com/shivamb/netflix-shows It contains Netflix and IMDB movie titles that can be matched against each other. Where IMDB has 80852 movie titles and Netflix has 6172 movie titles. You can use them as follows: from polyfuzz import PolyFuzz from polyfuzz.datasets import load_movie_titles data = load_movie_titles () model = PolyFuzz ( \"TF-IDF\" ) . match ( data [ \"Netflix\" ], data [ \"IMDB\" ]) Company Names \u00b6 This data is retrieved from https://www.kaggle.com/dattapiy/sec-edgar-companies-list?select=sec__edgar_company_info.csv and contains 100_000 company names to be matched against each other. This is a different use case than what you have typically seen so far. We often see two different lists compared with each other. Here, you can use this dataset to compare the company names with themselves in order to clean them up. You can use them as follows: from polyfuzz import PolyFuzz from polyfuzz.datasets import load_company_names data = load_company_names () model = PolyFuzz ( \"TF-IDF\" ) . match ( data , data ) PolyFuzz will recognize that the lists are similar and that you are looking to match the titles with themselves. It will ignore any comparison a string has with itself, otherwise everything will get mapped to itself.","title":"Datasets"},{"location":"tutorial/datasets/datasets/#datasets","text":"There are two datasets prepared for you to play around with: * Company Names * Movie Titles","title":"Datasets"},{"location":"tutorial/datasets/datasets/#movie-titles","text":"This data is retrieved from: https://www.kaggle.com/stefanoleone992/imdb-extensive-dataset https://www.kaggle.com/shivamb/netflix-shows It contains Netflix and IMDB movie titles that can be matched against each other. Where IMDB has 80852 movie titles and Netflix has 6172 movie titles. You can use them as follows: from polyfuzz import PolyFuzz from polyfuzz.datasets import load_movie_titles data = load_movie_titles () model = PolyFuzz ( \"TF-IDF\" ) . match ( data [ \"Netflix\" ], data [ \"IMDB\" ])","title":"Movie Titles"},{"location":"tutorial/datasets/datasets/#company-names","text":"This data is retrieved from https://www.kaggle.com/dattapiy/sec-edgar-companies-list?select=sec__edgar_company_info.csv and contains 100_000 company names to be matched against each other. This is a different use case than what you have typically seen so far. We often see two different lists compared with each other. Here, you can use this dataset to compare the company names with themselves in order to clean them up. You can use them as follows: from polyfuzz import PolyFuzz from polyfuzz.datasets import load_company_names data = load_company_names () model = PolyFuzz ( \"TF-IDF\" ) . match ( data , data ) PolyFuzz will recognize that the lists are similar and that you are looking to match the titles with themselves. It will ignore any comparison a string has with itself, otherwise everything will get mapped to itself.","title":"Company Names"},{"location":"tutorial/grouper/grouper/","text":"Custom Grouper \u00b6 The basic grouper is a TF-IDF implementation that uses single linkage to group the strings you mapped to together. With the customizability philosophy of PolyFuzz in mind it is not unexpected that you can also use any of the models, and even custom models, as your grouper! Here, we use Edit Distance instead of TF-IDF to group the strings we mapped to: from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] model = PolyFuzz ( \"TF-IDF\" ) . match ( from_list , to_list ) # Custom grouper base_edit_grouper = EditDistance ( n_jobs = 1 ) model . group ( base_edit_grouper ) And that is it! We have now grouped our matches we mapped to together using Edit Distance instead of TF-IDF.","title":"Custom Grouper"},{"location":"tutorial/grouper/grouper/#custom-grouper","text":"The basic grouper is a TF-IDF implementation that uses single linkage to group the strings you mapped to together. With the customizability philosophy of PolyFuzz in mind it is not unexpected that you can also use any of the models, and even custom models, as your grouper! Here, we use Edit Distance instead of TF-IDF to group the strings we mapped to: from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] model = PolyFuzz ( \"TF-IDF\" ) . match ( from_list , to_list ) # Custom grouper base_edit_grouper = EditDistance ( n_jobs = 1 ) model . group ( base_edit_grouper ) And that is it! We have now grouped our matches we mapped to together using Edit Distance instead of TF-IDF.","title":"Custom Grouper"},{"location":"tutorial/models/models/","text":"Models \u00b6 Currently, the following models are implemented in PolyFuzz: 1. TF-IDF 2. EditDistance with RapidFuzz 3. FastText and GloVe 4. \ud83e\udd17 Transformers With Flair , we can use all \ud83e\udd17 Transformers that are publicly available . We simply have to instantiate any Flair WordEmbedding method and pass it through PolyFuzzy. All models listed above can be found in polyfuzz.models and can be used to create and compare different matchers. TF-IDF \u00b6 Although the terms in TF-IDF are usually words, we are going to be using TF-IDF on a character-level. We will be extracting n-grams from a string and count the frequency of these n-grams across all input strings. For example, with 3-grams, the word \"hotel\" can be defined as \"hot\", \"ote\", and \"tel\". After generating the n-grams and applying TF-IDF on these \"terms\", we can use cosine similarity to compare the generated TF-IDF vectors against each other. We simply load in TFIDF from polyfuzz.models and pass it to our PolyFuzz instance: from polyfuzz.models import TFIDF from polyfuzz import PolyFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , matcher_id = \"TF-IDF\" ) model = PolyFuzz ( tfidf ) . match ( from_list , to_list ) and that's it! You can play around with the TFIDF matcher until you get the results you are looking for. Note that if you increase the min_similarity there is a chance that some strings will not be matched at all. EditDistance \u00b6 There are many edit distance functions one could use and the EditDistance model from polyfuzz.models allows you to pass in any distance function. As long as that distance function takes in two strings and spits out a float, you can pass anything! In the example below, we are going to be using Jaro Winkler Similarity from the jellyfish package to create our custom scorer: from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance from jellyfish import jaro_winkler_similarity from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] jellyfish_matcher = EditDistance ( n_jobs = 1 , scorer = jaro_winkler_similarity ) model = PolyFuzz ( jellyfish_matcher ) . match ( from_list , to_list ) RapidFuzz \u00b6 Edit distance measures are typically quite slow. Moreover, the one that is heavily used, fuzzywuzzy , has a very restrictive licence (GPL). Instead, I decided to create a RapidFuzz matcher which is a fast implementation of fuzzywuzzy and has a less restrictive licence (MIT): from polyfuzz import PolyFuzz from polyfuzz.models import RapidFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] rapidfuzz_matcher = RapidFuzz ( n_jobs = 1 ) model = PolyFuzz ( rapidfuzz_matcher ) . match ( from_list , to_list ) Embeddings \u00b6 With Flair , we can use all \ud83e\udd17 Transformers that are publicly available . The embeddings that are created are compared with cosine similarity in order to understand how similar the created embeddings are to each other. We simply have to instantiate any Flair WordEmbedding method and pass it through PolyFuzzy: from polyfuzz import PolyFuzz from polyfuzz.models import Embeddings from flair.embeddings import TransformerWordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 ) models = PolyFuzz ( bert_matcher ) . match ( from_list , to_list ) Flair allows you to use pool word embeddings to create more powerful word embeddings. Below, we pool FastText and BERT to create a single embedding representation from which we can calculate the similarity between strings: from polyfuzz import PolyFuzz from polyfuzz.models import Embeddings from flair.embeddings import TransformerWordEmbeddings , WordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 ) fasttext = WordEmbeddings ( 'en-crawl' ) fasttext_matcher = Embeddings ( fasttext , min_similarity = 0 ) matchers = [ bert_matcher , fasttext_matcher ] models = PolyFuzz ( matchers ) . match ( from_list , to_list ) Using Multiple Models \u00b6 from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance , TFIDF , Embeddings from flair.embeddings import TransformerWordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 , matcher_id = \"BERT\" ) tfidf_matcher = TFIDF ( min_similarity = 0 ) edit_matcher = EditDistance () matchers = [ bert_matcher , tfidf_matcher , edit_matcher ] models = PolyFuzz ( matchers ) . match ( from_list , to_list ) To access the results, we again can call get_matches but since we have multiple models we get back a dictionary of dataframes back. In order to access the results of a specific model, call get_matches with the correct id: >>> models . get_matches ( \"BERT\" ) From To Similarity 0 apple apple 1.000000 1 apples apples 1.000000 2 appl apple 0.928045 3 recal apples 0.825268 4 house mouse 0.887524 5 similarity mouse 0.791548 Finally, visualize the results to compare the models: models . visualize_precision_recall ( kde = True )","title":"Models"},{"location":"tutorial/models/models/#models","text":"Currently, the following models are implemented in PolyFuzz: 1. TF-IDF 2. EditDistance with RapidFuzz 3. FastText and GloVe 4. \ud83e\udd17 Transformers With Flair , we can use all \ud83e\udd17 Transformers that are publicly available . We simply have to instantiate any Flair WordEmbedding method and pass it through PolyFuzzy. All models listed above can be found in polyfuzz.models and can be used to create and compare different matchers.","title":"Models"},{"location":"tutorial/models/models/#tf-idf","text":"Although the terms in TF-IDF are usually words, we are going to be using TF-IDF on a character-level. We will be extracting n-grams from a string and count the frequency of these n-grams across all input strings. For example, with 3-grams, the word \"hotel\" can be defined as \"hot\", \"ote\", and \"tel\". After generating the n-grams and applying TF-IDF on these \"terms\", we can use cosine similarity to compare the generated TF-IDF vectors against each other. We simply load in TFIDF from polyfuzz.models and pass it to our PolyFuzz instance: from polyfuzz.models import TFIDF from polyfuzz import PolyFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , matcher_id = \"TF-IDF\" ) model = PolyFuzz ( tfidf ) . match ( from_list , to_list ) and that's it! You can play around with the TFIDF matcher until you get the results you are looking for. Note that if you increase the min_similarity there is a chance that some strings will not be matched at all.","title":"TF-IDF"},{"location":"tutorial/models/models/#editdistance","text":"There are many edit distance functions one could use and the EditDistance model from polyfuzz.models allows you to pass in any distance function. As long as that distance function takes in two strings and spits out a float, you can pass anything! In the example below, we are going to be using Jaro Winkler Similarity from the jellyfish package to create our custom scorer: from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance from jellyfish import jaro_winkler_similarity from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] jellyfish_matcher = EditDistance ( n_jobs = 1 , scorer = jaro_winkler_similarity ) model = PolyFuzz ( jellyfish_matcher ) . match ( from_list , to_list )","title":"EditDistance"},{"location":"tutorial/models/models/#rapidfuzz","text":"Edit distance measures are typically quite slow. Moreover, the one that is heavily used, fuzzywuzzy , has a very restrictive licence (GPL). Instead, I decided to create a RapidFuzz matcher which is a fast implementation of fuzzywuzzy and has a less restrictive licence (MIT): from polyfuzz import PolyFuzz from polyfuzz.models import RapidFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] rapidfuzz_matcher = RapidFuzz ( n_jobs = 1 ) model = PolyFuzz ( rapidfuzz_matcher ) . match ( from_list , to_list )","title":"RapidFuzz"},{"location":"tutorial/models/models/#embeddings","text":"With Flair , we can use all \ud83e\udd17 Transformers that are publicly available . The embeddings that are created are compared with cosine similarity in order to understand how similar the created embeddings are to each other. We simply have to instantiate any Flair WordEmbedding method and pass it through PolyFuzzy: from polyfuzz import PolyFuzz from polyfuzz.models import Embeddings from flair.embeddings import TransformerWordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 ) models = PolyFuzz ( bert_matcher ) . match ( from_list , to_list ) Flair allows you to use pool word embeddings to create more powerful word embeddings. Below, we pool FastText and BERT to create a single embedding representation from which we can calculate the similarity between strings: from polyfuzz import PolyFuzz from polyfuzz.models import Embeddings from flair.embeddings import TransformerWordEmbeddings , WordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 ) fasttext = WordEmbeddings ( 'en-crawl' ) fasttext_matcher = Embeddings ( fasttext , min_similarity = 0 ) matchers = [ bert_matcher , fasttext_matcher ] models = PolyFuzz ( matchers ) . match ( from_list , to_list )","title":"Embeddings"},{"location":"tutorial/models/models/#using-multiple-models","text":"from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance , TFIDF , Embeddings from flair.embeddings import TransformerWordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 , matcher_id = \"BERT\" ) tfidf_matcher = TFIDF ( min_similarity = 0 ) edit_matcher = EditDistance () matchers = [ bert_matcher , tfidf_matcher , edit_matcher ] models = PolyFuzz ( matchers ) . match ( from_list , to_list ) To access the results, we again can call get_matches but since we have multiple models we get back a dictionary of dataframes back. In order to access the results of a specific model, call get_matches with the correct id: >>> models . get_matches ( \"BERT\" ) From To Similarity 0 apple apple 1.000000 1 apples apples 1.000000 2 appl apple 0.928045 3 recal apples 0.825268 4 house mouse 0.887524 5 similarity mouse 0.791548 Finally, visualize the results to compare the models: models . visualize_precision_recall ( kde = True )","title":"Using Multiple Models"},{"location":"tutorial/quickstart/quickstart/","text":"Installation \u00b6 You can install PolyFuzz via pip: pip install polyfuzz This will install the base dependencies and excludes any deep learning/embedding models. If you want to be making use of \ud83e\udd17 Transformers, install the additional additional Flair dependency: pip install polyfuzz [ flair ] Getting Started \u00b6 The main goal of PolyFuzz is to allow the user to perform different methods for matching strings. We start by defining two lists, one to map from and one to map to. We are going to be using TF-IDF to create n-grams on a character level in order to compare similarity between strings. We only have to instantiate PolyFuzz with TF-IDF and match the lists: from polyfuzz import PolyFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] model = PolyFuzz ( \"TF-IDF\" ) . match ( from_list , to_list ) The resulting matches can be accessed through model.get_matches() : >>> model . get_matches () From To Similarity 0 apple apple 1.000000 1 apples apples 1.000000 2 appl apple 0.783751 3 recal None 0.000000 4 house mouse 0.587927 5 similarity None 0.000000 NOTE : When instantiating PolyFuzz we also could have used \"EditDistance\" or \"Embeddings\" to quickly access Levenshtein and FastText (English) respectively. Group Matches \u00b6 We can group the matches To as there might be significant overlap in strings in our to_list. To do this, we calculate the similarity within strings in to_list and use single linkage to then group the strings with a high similarity. When we extract the new matches, we can see an additional column Group in which all the To matches were grouped to: >>> model . group ( link_min_similarity = 0.75 ) >>> model . get_matches () From To Similarity Group 0 apple apple 1.000000 apples 1 apples apples 1.000000 apples 2 appl apple 0.783751 apples 3 recal None 0.000000 None 4 house mouse 0.587927 mouse 5 similarity None 0.000000 None As can be seen above, we grouped apple and apples together to apple such that when a string is mapped to apple it will fall in the cluster of [apples, apple] and will be mapped to the first instance in the cluster which is apples . Precision-Recall Curve \u00b6 Next, we would like to see how well our model is doing on our data. We express our results as precision and recall where precision is defined as the minimum similarity score before a match is correct and recall the percentage of matches found at a certain minimum similarity score. Creating the visualizations is as simple as: model.visualize_precision_recall()","title":"Quickstart"},{"location":"tutorial/quickstart/quickstart/#installation","text":"You can install PolyFuzz via pip: pip install polyfuzz This will install the base dependencies and excludes any deep learning/embedding models. If you want to be making use of \ud83e\udd17 Transformers, install the additional additional Flair dependency: pip install polyfuzz [ flair ]","title":"Installation"},{"location":"tutorial/quickstart/quickstart/#getting-started","text":"The main goal of PolyFuzz is to allow the user to perform different methods for matching strings. We start by defining two lists, one to map from and one to map to. We are going to be using TF-IDF to create n-grams on a character level in order to compare similarity between strings. We only have to instantiate PolyFuzz with TF-IDF and match the lists: from polyfuzz import PolyFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] model = PolyFuzz ( \"TF-IDF\" ) . match ( from_list , to_list ) The resulting matches can be accessed through model.get_matches() : >>> model . get_matches () From To Similarity 0 apple apple 1.000000 1 apples apples 1.000000 2 appl apple 0.783751 3 recal None 0.000000 4 house mouse 0.587927 5 similarity None 0.000000 NOTE : When instantiating PolyFuzz we also could have used \"EditDistance\" or \"Embeddings\" to quickly access Levenshtein and FastText (English) respectively.","title":"Getting Started"},{"location":"tutorial/quickstart/quickstart/#group-matches","text":"We can group the matches To as there might be significant overlap in strings in our to_list. To do this, we calculate the similarity within strings in to_list and use single linkage to then group the strings with a high similarity. When we extract the new matches, we can see an additional column Group in which all the To matches were grouped to: >>> model . group ( link_min_similarity = 0.75 ) >>> model . get_matches () From To Similarity Group 0 apple apple 1.000000 apples 1 apples apples 1.000000 apples 2 appl apple 0.783751 apples 3 recal None 0.000000 None 4 house mouse 0.587927 mouse 5 similarity None 0.000000 None As can be seen above, we grouped apple and apples together to apple such that when a string is mapped to apple it will fall in the cluster of [apples, apple] and will be mapped to the first instance in the cluster which is apples .","title":"Group Matches"},{"location":"tutorial/quickstart/quickstart/#precision-recall-curve","text":"Next, we would like to see how well our model is doing on our data. We express our results as precision and recall where precision is defined as the minimum similarity score before a match is correct and recall the percentage of matches found at a certain minimum similarity score. Creating the visualizations is as simple as: model.visualize_precision_recall()","title":"Precision-Recall Curve"}]}